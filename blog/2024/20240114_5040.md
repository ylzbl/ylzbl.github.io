# 阿里通义实验室回应关于“全民舞王”的一切

*James|2024-01-14|AIGC*

作者|James

继妙鸭相机之后，阿里又一个AIGC原生小程序火了。

下载通义千问App，输入“全民舞王”之后，就可以用一张清晰的正面全身照片，生成几秒钟的跳舞视频，从蒙古舞到科目三应有尽有。“兵马俑跳科目三”登上了一周前的社交媒体热搜，在海外的社交网络上，也有不少不能及时用到的人实名表示羡慕。

![Image](https://p3-sign.toutiaoimg.com/tos-cn-i-axegupay5k/72f157df4f934adabfe8bc5e8aec8cd5~noop.image?_iz=58558&from=article.pc_detail&lk3s=953192f4&x-expires=1705847429&x-signature=9tq3jEWnsrO2DqGKWE5AGN7TqTY%3D)

“全民舞王”背后用到的技术正是由阿里通义实验室开发的Animate Anyone算法，该算法在上个月首先在GitHub公开了论文，其中已经提到了一些和历史上同类算法的对比细节，其完善程度令人震惊。

Animate Anyone官宣的时刻，正值其他文生视频算法和产品批量涌现，国人主导的Pika 1.0惊艳开启内测，字节跳动的Magic Animate和微软的GAIA同样在展示效果上明显升级。

阿里通义实验室还有另外两款最新算法官宣，其中Outfit Anyone可以给模特一键换装，AnyText解决了文章图当中字体，特别是对于汉字生成效果不准确的问题。

文生视频的动向受到文娱行业的密切关注。值此“全民舞王”爆火之际，娱乐资本论·视智未来率先联系到阿里通义实验室的团队负责人，详细的帮我们解答了有关其三款最新算法的一些大家关心的问题。

![Image](https://p3-sign.toutiaoimg.com/tos-cn-i-twdt4qpehh/7af033e4ea58437a9deb06c56b0f8d7c~noop.image?_iz=58558&from=article.pc_detail&lk3s=953192f4&x-expires=1705847429&x-signature=VMyC4kcpLdB4aU60mwMnQHpDPtA%3D)

---以下为采访实录---

视智未来：在舞王上线后的这几天，你们的统计数据当中有什么亮点和有趣的地方？

目前最受欢迎的模板是科目三，这与全球的趋势相吻合，科目三模板断崖领先于其他模板。

目前我们观察到用户使用频率在某些时间段会显著增加。例如，中午12点半之后到大约一点多的时间段，用户使用频率较高。晚上下班后，使用量逐渐上升，直至大约8-9点钟达到另一个高峰。

人们各自用真人，或者是非真人比如动漫人物或兵马俑的照片来制作。我们后台理论上可以识别这些种类的占比，但目前还没有具体的数据。

同时在线人数峰值属于内部数据，我们目前不对外公布。但可以确认的是，在高峰期间，等待时间确实有所增长。平时是15分钟，晚上8-9点时有用户反馈等待时间超过了20分钟。

另外，舞王产品当前只在通义千问App中使用，但我们也在考虑是否将其扩展到阿里系的其他平台上。目前还没有具体的计划。

![Image](https://p3-sign.toutiaoimg.com/tos-cn-i-twdt4qpehh/d70b9764275c4707b09f43e6138246f6~noop.image?_iz=58558&from=article.pc_detail&lk3s=953192f4&x-expires=1705847429&x-signature=5YPgyy1CvK1wZQ8G2k1953WK2qE%3D)

视智未来：Animate Anyone在论文中使用了TikTok的一些网红跳舞的片段。国外媒体普遍关注的是，它是否是使用TikTok数据训练的，是否已经获得了这些网红的授权？

在研发阶段，我们主要使用的是自己内部的数据集。当然，在论文中的评测集对比上，我们对比了几个主流的人物视频生成模型。在进行这样的对比时，我们可能会使用一些公开的数据集，比如TikTok的，以评估哪家的技术效果更好。这些数据集在行业内被广泛认可，但实际上在开发过程中，我们是基于自己的数据集训练。

视智未来：你们自己的数据集是来自淘宝上的商品描述视频，或者是淘宝直播的片段吗？

是的，我们有自己的内部数据集。

视智未来：目前在舞王当中可选的动作是固定的，为什么考虑设置有限且固定的动作？

算法本身，对于符合格式的动作，都是支持的。设计有限且固定的动作，主要是从产品体验角度出发，通义千问App面对的是全网的手机用户，对这些用户而言，自己拍摄、制作动作，是有难度的。这种复杂操作，对于开发者更合适一些。

我们后续会增加更多有趣的舞蹈模板。同时，我们也在思考如何能兼顾用户自定义动作的需求和产品的易用性。

视智未来：支不支持从一个视频的动作提取出来转换到另一个视频？比如提取一个视频中主角的动作节奏，然后套到另一个图像上？

和上一个问题类似，从视频提取动作技术上是可行的。但是做到产品上，还需要更多考虑。

视智未来：舞王生成一次的十几分钟包含了排队时间，那么单算Animate Anyone算法本身的生成效率如何？

这个取决于很多因素：输入图的分辨率、后台机器的型号、生成的时长等。

我们测试过，Animate算法的效率和常见的VideoComposer等视频生成，效率基本一致。现在产品上，不排队情况下，10秒左右的视频，生成时间在9分钟左右。

![Image](https://p3-sign.toutiaoimg.com/tos-cn-i-twdt4qpehh/0abaac0f2938467bbbfc8ad29083557f~noop.image?_iz=58558&from=article.pc_detail&lk3s=953192f4&x-expires=1705847429&x-signature=bk6eAVuKveewx128Wf9upw%2B6Vls%3D)

视智未来：据我所知，你们的方法是找出人物的关键点，然后将其匹配到类似火柴棍的模型上，对吗？

确实如此。我们的模板相当于是骨架提取。用户上传照片时，我们的算法会提取照片中的基本信息，如人脸、表情、服装和背景，并将这些与模板骨架结合，让其动起来。

有人使用柯南漫画封面进行生成，生成后封面背景的文字没有错位。这说明背景在识别后保持不变。这也是我们技术上的一个优势。在GitHub上的展示案例中，我们的算法同样展示出能够保持背景稳定的明显的优势。

视智未来：我想知道是否可以将一个视频的最后一帧作为下一个视频的第一帧。会不会有接缝感觉？

与Pika有所不同，由于舞蹈背后的骨架可以设定为连贯的动作，我们认为可以实现无缝拼接。

视智未来：之前论文说一些大动作可能会有问题，能大致说一下是哪些类型的动作现在还不完善吗？

转身是一个挑战，这是整个行业的普遍问题。对于快节奏的转身动作和武打动作，因为这些动作幅度大，节奏快，目前的算法可能无法完美处理这些动作，可能会出现一些瑕疵。

我们需要对算法进行进一步的优化和升级。数据集是一个方面，我们可能需要一整套数据，而不仅仅是一两段视频。另外算法的结构设计也需要升级和优化。

视智未来：今年开发团队对于Animate Anyone和“全民舞王”的下一步改进计划有哪些？

算法上，会持续提升算法的细节效果，以及扩展更多功能，比如支持相机、背景控制，支持多人同时控制等。

产品上，今年计划加入的新功能，至少包括多人共舞和上传半身照片。

即使使用现有技术，也可以在绿幕之下先把每一个单人的动作做出来，再拼接到一起，甚至像爱尔兰的《大河之舞》那种也可以做。但我们接下来希望在Animate Anyone的算法内部来实现多人共舞的功能，一次生成而不需要后期再加工。

另外我们现在研发上传半身照片就可以生成，目前是需要上传正面的全身照片才行。

![Image](https://p3-sign.toutiaoimg.com/tos-cn-i-twdt4qpehh/6c7981a721ee4781ae104e0435bc674e~noop.image?_iz=58558&from=article.pc_detail&lk3s=953192f4&x-expires=1705847429&x-signature=MX8%2FFyLUq%2BgtR%2BqfezzJ5JHBOT0%3D)

视智未来：在AnyText出来之前，也有很多人会自然的想到将AI生图和生成文字作为两个单独的流程来处理。但文字可能难以准确的叠加在同样的位置上，而且还要擦除上面原来的字。这方面的技术难题是怎样攻克的呢？

分成两个单独流程处理是最直观的方式，但也使问题更复杂。比如AI生图后，生成的伪文字的位置很难定位，通用OCR模型无法检测到，也就无法确定接下来生成文字的位置。而且为了提升视觉一致性，文字的风格与图像应保持一致而不是单独处理。

因此我们设计了一步图文融合的方案。一方面我们将待生成文字的字形，位置等信息加到图像隐空间，在常规文生图的流程中加入了生成文字的“催化剂”；另一方面，待生成的文字受分词器影响无法将每个字符对应到单独的token，且预训练的token特征以语义编码为主，无法提供足够的笔画信息，我们采用OCR模型对生成文本单独编码，再和提示词的其他描述内容的语义做融合。

基于这两点，我们做到了在图像中生成笔画准确、视觉风格一致的文字。

视智未来：我们发现针对中文的招牌字体优化程度非常好，这是因为测试数据集的筛选和优化有针对性吗？

没有刻意做优化，具体的数据集信息可以参考我们的论文。

![Image](https://p3-sign.toutiaoimg.com/tos-cn-i-twdt4qpehh/685cdc4a56bb41d9b0222bc92fa08c18~noop.image?_iz=58558&from=article.pc_detail&lk3s=953192f4&x-expires=1705847429&x-signature=pkboQi44SdmRUt4JQQHvqIIChuc%3D)

视智未来：AnyText即使是针对3D造型，浮雕效果的汉字生成效果也是可以的，在这方面有没有克服什么特殊的困难？

这一点是符合预期的，我们对用户输入的提示词进行了解耦。

如：【一个大理石浮雕，上面写着“AnyText”】。对引号外的字符，它们和常规文生图流程一样，经过文本编码器抽取语义特征；对引号内的字符，我们认为其语义本身并无价值，而每个字符的笔画尤为重要，因此引入预训练OCR模型提取字形特征，之后语义和字形特征经过注意力机制做融合，就可以使“AnyText”这几个字符在生成时关注图像的一致性，从而实现浮雕的效果。

视智未来：演示照片当中，生成的文字角度都是比较正的，也就是直视角度。其它角度会不那么准确吗？

除直视角度，AnyText也可以在弯曲、不规则形状、折角、透视变换等区域生成文字，不过这种情况的文字应该会带来一定准确率的降低，因为训练数据中大部分为直视角度。

视智未来：除中文和英文之外，还有其他哪些语言文字经过了相应优化？

AnyText方案本身并未限定语言，但目前训练数据中绝大部分为中文和英文，其他语言并未做特别优化，后续会考虑逐步扩大多语言训练集，也会借助开源生态的力量。

视智未来：在生成的时候可以在prompt（提示词）当中选择字体吗？

目前的版本不能通过prompt指定字体，文字生成模式下，字体的选择主要受图像描述的场景影响，在尽可能不破坏视觉一致性的前提下随机生成某种字体的文字。

不只是跳舞和写字，还有换装

视智未来：我们知道在Outfit Anyone之前，早在去年三四月份，市场上已经有一些公司在尝试优化和魔改Stable Diffusion，使其适应电商模特换装场景。Outfit Anyone相对的优越性在哪里？

确实利用SD魔改的很多试衣的应用大部分是基于SD的mask

 inpainting技术，通常是基于一张已有的模特上身图或者人台图片, 

保持衣服区域不变，重新生成人头和人脸的区域。这种方式对于输入图片的要求很高，同时由于不需要对服饰区域进行修改，生成难度相对较低。

而Outfit

 

Anyone对于图片输入的要求很低，仅需要输入服饰平铺图，就可以实现服饰的上身效果的生成。这种方式减少用户的使用成本，能够扩展更多的应用场景中。此外，Outfit

 Anyone 也能够支持服饰搭配（上下装组合），身材试衣等应用场景，同时加入了refiner，可以做到更好的一致性。

![Image](https://p3-sign.toutiaoimg.com/tos-cn-i-twdt4qpehh/73b4ae146c0749ef96eb59d5f8bddc6c~noop.image?_iz=58558&from=article.pc_detail&lk3s=953192f4&x-expires=1705847429&x-signature=XEghFpFhKvUl4zoQjr5XkinOtuk%3D)

视智未来：Outfit Anyone会在近期有什么实际应用落地吗？

我们会和阿里巴巴内部的淘宝电商场景结合，也会探索一些外部的应用场景。

此外，在我们的论文中也展示了，Animate Anyone和Outfit Any One算法可以叠加使用,模特在生成舞蹈视频之前可以先换装，然后再跳舞。

视智未来：那么，你们是否考虑在舞王应用中加入这一环节，比如先换装再跳舞？

从产品角度看，可能不会很快实现，但这在论文和项目页面上已有相关展示，是一个值得期待的未来应用场景。

请问你们在开始构思和设计的时候，是确定要应用到淘宝的电商场景，还是有其他的初衷？

目前，我们这三款产品都属于通义实验室。Outfit Anyone更紧密地结合了电商、时尚场景，具有换装能力。AnyText这款产品则偏向图像生成，比之前的文字生成能力有所加强。这些产品都是通义实验室最近发布的模型算法的一部分。

通义实验室主要集中于通义系列大模型的开发，这三个项目实战意义非常强，与实际业务应用的联系特别紧密，不仅仅是纯技术探索。这也是因为当前的大模型在各种应用场景中都非常明显。至少从我们开发的Animate

 Anyone来看，这个项目的起源仍然基于技术上的突破。我们在生成场景上根据实际需要进行了一些创新，比如舞王。

视智未来：整个开发过程从刚开始到发布论文用了多久？这是通义团队的主要项目，还是在业余时间做出的项目？

Animate

 

Anyone这项研究工作开展的比较早，大约2023年9月份就开始了。到2023年11月底，我们公开了论文和项目主页，得到了同行的关注，有很多国内外的关注者自发转发，引起了广泛的讨论。之后我们开始筹备上线通义千问里的“全民舞王”这个功能，这两天上线后，也吸引了大量网友的体验。

研究本身并不局限在舞蹈生成，为了让偏枯燥的学术工作，变成大家都能体验、都能找到乐趣的功能。

视智未来：业界非常关注这三个模型是否可以开源，以及商业化使用生成结果，和进行二次开发。

AnyText使用的开源许可证是Apache License 2.0，不是只能在阿里系的场景里使用，你可以将其商业化，但需要遵守一定的限制和条件，具体请参阅许可证协议。

Animate Anyone和Outfit Anyone项目的代码目前还没有开源，但GitHub上已有相关技术文档。

目前Outfit Anyone和AnyText已经登陆huggingface和魔搭。Outfit Anyone在Huggingface和Modelscope体验页，不限制用户对于生成的试衣图片的使用。

视智未来：在自行架设相关模型体验的时候，上传照片如果有敏感的，算法端会处理吗？

我们的算法并不会屏蔽敏感图片，这种屏蔽是由前端限制实现的。

